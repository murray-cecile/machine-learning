{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also write a short report (~2 pages) that compares the performance of the different classifiers across all the metrics for the data set used in the last assignment. Which classifier does better on which metrics? How do the results change over time? What would be your recommendation to someone who's working on this model to identify 5% of posted projects to intervene with, which model should they decide to go forward with and deploy?\n",
    "\n",
    "The report should not be a list of graphs and numbers. It needs to explain to a policy audience the implications of your analysis and your recommendations as a memo you would send to a policy audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3: Predicting whether a donorschoose.org project will be funded within 60 days\n",
    "\n",
    "#### Cecile Murray\n",
    "\n",
    "DonorsChoose.org is a website that helps connect teachers in need of financial support for educational projects with a network of donors willing to fund those projects. Donors give any amount they choose; once a project is fully funded, the requested materials are delivered. I analyzed past project funding data to build a model predicting whether or not a given project will be fully funded within 60 days of being posted on DonorsChoose. \n",
    "\n",
    "[DESCRIBE FINDINGS]\n",
    "\n",
    "### Data overview\n",
    "\n",
    "I built predictive models using data on the individual project listings. Each listing includes some basic identifying information, geographic identifiers, and some fields describing the project's attributes.\n",
    "\n",
    "### Modeling approach\n",
    "\n",
    "In order to identify which model is likely to produce the best predictions, I built and tested a range of types of models. To evaluate the performance of the models, I split my data into three different training and testing datasets. In each case, the training data contained all the projects posted on DonorsChoose up until a certain date, and the testing data contained all the projects posted after that date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics\n",
    "\n",
    "I evaluated the performance of each model using a few standard metrics from machine learning:\n",
    "\n",
    "* **Baseline accuracy:** This metric represents the share of projects that the model classified correctly. That is, it takes the sum of projects predicted to be funded within 60 days that were fully funded in that time frame and the projects predicted not to be funded that were not funded, and divides by the total number of projects.\n",
    "\n",
    "* **Precision:** This metric is essentially a true positives rate. It describes what share of the projects that the model predicted would be funded within 60 days actually were funded in that time window.\n",
    "\n",
    "* **Recall:** This metric indicates the share of all the projects that actually were funded in time that the model picked up on. \n",
    "\n",
    "* **AUC-ROC:** [describe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
